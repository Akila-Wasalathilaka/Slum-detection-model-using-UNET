{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3727b445",
   "metadata": {},
   "source": [
    "# Global Slum Detector — Batch Generation (15–20 samples)\n",
    "Run this in Google Colab to generate overlays for a subset of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup (clone + install)\n",
    "!git clone https://github.com/Akila-Wasalathilaka/Slum-detection-model-using-UNET.git -q\n",
    "%cd Slum-detection-model-using-UNET\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa46340",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Configure paths\n",
    "from pathlib import Path\n",
    "import random, os, json, cv2, numpy as np\n",
    "from google.colab import files\n",
    "\n",
    "# Upload your checkpoint if not in repo\n",
    "CHECKPOINT = 'best_global_model.pth'  #@param {type: 'string'}\n",
    "IMAGES_DIR = 'data/test/images'       #@param {type: 'string'}\n",
    "OUTPUT_DIR = 'colab_outputs'          #@param {type: 'string'}\n",
    "N_SAMPLES = 20                         #@param {type: 'number'}\n",
    "\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "image_paths = sorted([str(p) for p in Path(IMAGES_DIR).glob('*.tif')] + [str(p) for p in Path(IMAGES_DIR).glob('*.png')] + [str(p) for p in Path(IMAGES_DIR).glob('*.jpg')])\n",
    "assert len(image_paths) > 0, 'No images found in data/test/images. Upload or mount your data.'\n",
    "\n",
    "subset = random.sample(image_paths, min(N_SAMPLES, len(image_paths)))\n",
    "len(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90d3dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Batch inference and overlay generation\n",
    "import cv2, numpy as np\n",
    "from global_slum_detector import GlobalSlumDetector\n",
    "\n",
    "detector = GlobalSlumDetector(CHECKPOINT)\n",
    "results = []\n",
    "for i, img_path in enumerate(subset, 1):\n",
    "    out = detector.predict_global(img_path, use_tta=True, use_tent=True, adaptive_threshold=True)\n",
    "    base = Path(img_path).stem\n",
    "    overlay_path = str(Path(OUTPUT_DIR)/f'{base}_overlay.jpg')\n",
    "    prob_path = str(Path(OUTPUT_DIR)/f'{base}_prob.jpg')\n",
    "    cv2.imwrite(overlay_path, cv2.cvtColor(out['overlay'], cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(prob_path, (out['probability']*255).astype(np.uint8))\n",
    "    results.append({\"image\": img_path, \"overlay\": overlay_path, \"prob\": prob_path, \"threshold\": float(out['threshold']), \"mean_conf\": float(out['confidence'].mean())})\n",
    "    print(f'[{i}/{len(subset)}] Saved: {overlay_path}')\n",
    "\n",
    "with open(Path(OUTPUT_DIR)/'batch_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print('Done. Results saved to', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bcb89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Download results as zip\n",
    "import shutil\n",
    "shutil.make_archive('overlays', 'zip', OUTPUT_DIR)\n",
    "files.download('overlays.zip')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
